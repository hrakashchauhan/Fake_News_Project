{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d4e1573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Worldwide Information Credibility Dataset Builder ---\n",
      "\n",
      "--- Processing Files ---\n",
      "\n",
      "Combining all processed data...\n",
      "\n",
      "--- Balancing the Dataset (Downsampling) ---\n",
      "Final Balanced Training Set Size: 160090\n",
      "CREDIBLE articles in final set: 80045\n",
      "MISINFORMATION articles in final set: 80045\n",
      "\n",
      "Saving the final, balanced training dataset...\n",
      "SUCCESS: File 'master_balanced_training_data.csv' saved!\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================================\n",
    "# THE DEFINITIVE DATA PREPARATION SCRIPT (FOR VS CODE)\n",
    "# This script combines all 27+ datasets into a single, clean, balanced master file.\n",
    "# ===================================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"--- Worldwide Information Credibility Dataset Builder ---\")\n",
    "\n",
    "# --- Step 1: Define File Lists Based on Our Strategy ---\n",
    "# We intentionally EXCLUDE test.csv and valid_data.csv from the training set.\n",
    "files_with_internal_labels = [\n",
    "    'fake_news_dataset.csv', 'train.csv', 'train_data.csv', 'news_dataset.csv'\n",
    "]\n",
    "files_to_label_as_credible = {\n",
    "    'True.csv': ['title', 'text'], 'abcnews-date-text.csv': ['headline_text'],\n",
    "    'articles.csv': ['text'], 'BBCHindi.csv': ['description'],\n",
    "    'bbc-news-data.csv': ['content'], 'business_data.csv': ['text'],\n",
    "    'DainikBhaskar.csv': ['content'], 'DataSet_Misinfo_TRUE.csv': ['text'],\n",
    "    'education_data.csv': ['text'], 'entertainment_data.csv': ['text'],\n",
    "    'IndianFinancialNews.csv': ['Title', 'Description'], 'JagranNews.csv': ['content'],\n",
    "    'kosmopulse_articles_with_entities.csv': ['text'], 'Live_Hindustan.csv': ['content'],\n",
    "    'NavbharatTimes.csv': ['content'], 'news_articles.csv': ['text'],\n",
    "    'news_summary.csv': ['text'], 'news_summary_more.csv': ['headlines', 'text'],\n",
    "    'sports_data.csv': ['text'], 'technology_data.csv': ['text'],\n",
    "}\n",
    "files_to_label_as_misinformation = {\n",
    "    'Fake.csv': ['title', 'text'], 'EXTRA_RussianPropagandaSubset.csv': ['content'],\n",
    "    'DataSet_Misinfo_FAKE.csv': ['text']\n",
    "}\n",
    "\n",
    "all_dataframes = []\n",
    "processed_log = []\n",
    "error_log = []\n",
    "\n",
    "# --- Step 2: Processing Logic ---\n",
    "def process_file(filepath, label, text_columns):\n",
    "    global all_dataframes, processed_log, error_log\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, on_bad_lines='skip')\n",
    "        df['label'] = label\n",
    "        df['text'] = df[text_columns].fillna('').agg(' '.join, axis=1)\n",
    "        all_dataframes.append(df[['text', 'label']])\n",
    "        processed_log.append(filepath)\n",
    "    except Exception as e:\n",
    "        error_log.append((filepath, e))\n",
    "\n",
    "print(\"\\n--- Processing Files ---\")\n",
    "for fp, cols in files_to_label_as_credible.items(): process_file(fp, 1, cols)\n",
    "for fp, cols in files_to_label_as_misinformation.items(): process_file(fp, 0, cols)\n",
    "for fp in files_with_internal_labels:\n",
    "    try:\n",
    "        df = pd.read_csv(fp, on_bad_lines='skip')\n",
    "        if 'label' in df.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df['label']):\n",
    "                df['label'] = df['label'].apply(lambda x: 1 if x == 1 else 0)\n",
    "            else:\n",
    "                df['label'] = df['label'].astype(str).str.lower().apply(lambda x: 0 if 'fake' in x else 1)\n",
    "            text_parts = []\n",
    "            for col in ['title', 'author', 'text']:\n",
    "                if col in df.columns: text_parts.append(df[col].fillna(''))\n",
    "            df['text'] = pd.concat(text_parts, axis=1).agg(' '.join, axis=1)\n",
    "            all_dataframes.append(df[['text', 'label']])\n",
    "            processed_log.append(fp)\n",
    "        else:\n",
    "            error_log.append((fp, \"No 'label' column found\"))\n",
    "    except Exception as e:\n",
    "        error_log.append((fp, e))\n",
    "\n",
    "# --- Step 3: Combine, Clean, and Balance ---\n",
    "if not all_dataframes:\n",
    "    print(\"\\nCRITICAL: No data was processed. Please check file names.\")\n",
    "else:\n",
    "    print(\"\\nCombining all processed data...\")\n",
    "    df_master = pd.concat(all_dataframes, ignore_index=True)\n",
    "    df_master.dropna(subset=['text'], inplace=True)\n",
    "    df_master = df_master[df_master['text'].str.strip() != '']\n",
    "    df_master.drop_duplicates(subset=['text'], inplace=True)\n",
    "    \n",
    "    print(\"\\n--- Balancing the Dataset (Downsampling) ---\")\n",
    "    misinfo_df = df_master[df_master['label'] == 0]\n",
    "    credible_df = df_master[df_master['label'] == 1]\n",
    "    \n",
    "    if len(credible_df) > len(misinfo_df):\n",
    "        credible_downsampled_df = credible_df.sample(n=len(misinfo_df), random_state=42)\n",
    "        df_balanced = pd.concat([misinfo_df, credible_downsampled_df])\n",
    "    else: # In case there's more misinformation, which is unlikely but safe to handle\n",
    "        misinfo_downsampled_df = misinfo_df.sample(n=len(credible_df), random_state=42)\n",
    "        df_balanced = pd.concat([misinfo_downsampled_df, credible_df])\n",
    "\n",
    "    df_final_training = df_balanced.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Final Balanced Training Set Size: {len(df_final_training)}\")\n",
    "    print(f\"CREDIBLE articles in final set: {df_final_training['label'].value_counts().get(1, 0)}\")\n",
    "    print(f\"MISINFORMATION articles in final set: {df_final_training['label'].value_counts().get(0, 0)}\")\n",
    "\n",
    "    # --- Step 4: Save the Final Dataset ---\n",
    "    print(\"\\nSaving the final, balanced training dataset...\")\n",
    "    df_final_training.to_csv('master_balanced_training_data.csv', index=False)\n",
    "    print(\"SUCCESS: File 'master_balanced_training_data.csv' saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a8d9020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Worldwide Information Credibility Dataset Builder ---\n",
      "\n",
      "--- Processing Files ---\n",
      "\n",
      "Combining all processed data...\n",
      "\n",
      "--- Balancing the Dataset (Downsampling) ---\n",
      "Final Balanced Training Set Size: 160090\n",
      "CREDIBLE articles in final set: 80045\n",
      "MISINFORMATION articles in final set: 80045\n",
      "\n",
      "Saving the final, balanced training dataset...\n",
      "SUCCESS: File 'master_balanced_training_data.csv' saved!\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================================\n",
    "# THE DEFINITIVE DATA PREPARATION SCRIPT (FOR VS CODE)\n",
    "# This script combines all 27+ datasets into a single, clean, balanced master file.\n",
    "# ===================================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"--- Worldwide Information Credibility Dataset Builder ---\")\n",
    "\n",
    "# --- Step 1: Define File Lists Based on Our Strategy ---\n",
    "# We intentionally EXCLUDE test.csv and valid_data.csv from the training set.\n",
    "files_with_internal_labels = [\n",
    "    'fake_news_dataset.csv', 'train.csv', 'train_data.csv', 'news_dataset.csv'\n",
    "]\n",
    "files_to_label_as_credible = {\n",
    "    'True.csv': ['title', 'text'], 'abcnews-date-text.csv': ['headline_text'],\n",
    "    'articles.csv': ['text'], 'BBCHindi.csv': ['description'],\n",
    "    'bbc-news-data.csv': ['content'], 'business_data.csv': ['text'],\n",
    "    'DainikBhaskar.csv': ['content'], 'DataSet_Misinfo_TRUE.csv': ['text'],\n",
    "    'education_data.csv': ['text'], 'entertainment_data.csv': ['text'],\n",
    "    'IndianFinancialNews.csv': ['Title', 'Description'], 'JagranNews.csv': ['content'],\n",
    "    'kosmopulse_articles_with_entities.csv': ['text'], 'Live_Hindustan.csv': ['content'],\n",
    "    'NavbharatTimes.csv': ['content'], 'news_articles.csv': ['text'],\n",
    "    'news_summary.csv': ['text'], 'news_summary_more.csv': ['headlines', 'text'],\n",
    "    'sports_data.csv': ['text'], 'technology_data.csv': ['text'],\n",
    "}\n",
    "files_to_label_as_misinformation = {\n",
    "    'Fake.csv': ['title', 'text'], 'EXTRA_RussianPropagandaSubset.csv': ['content'],\n",
    "    'DataSet_Misinfo_FAKE.csv': ['text']\n",
    "}\n",
    "\n",
    "all_dataframes = []\n",
    "processed_log = []\n",
    "error_log = []\n",
    "\n",
    "# --- Step 2: Processing Logic ---\n",
    "def process_file(filepath, label, text_columns):\n",
    "    global all_dataframes, processed_log, error_log\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, on_bad_lines='skip')\n",
    "        df['label'] = label\n",
    "        df['text'] = df[text_columns].fillna('').agg(' '.join, axis=1)\n",
    "        all_dataframes.append(df[['text', 'label']])\n",
    "        processed_log.append(filepath)\n",
    "    except Exception as e:\n",
    "        error_log.append((filepath, e))\n",
    "\n",
    "print(\"\\n--- Processing Files ---\")\n",
    "for fp, cols in files_to_label_as_credible.items(): process_file(fp, 1, cols)\n",
    "for fp, cols in files_to_label_as_misinformation.items(): process_file(fp, 0, cols)\n",
    "for fp in files_with_internal_labels:\n",
    "    try:\n",
    "        df = pd.read_csv(fp, on_bad_lines='skip')\n",
    "        if 'label' in df.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df['label']):\n",
    "                df['label'] = df['label'].apply(lambda x: 1 if x == 1 else 0)\n",
    "            else:\n",
    "                df['label'] = df['label'].astype(str).str.lower().apply(lambda x: 0 if 'fake' in x else 1)\n",
    "            text_parts = []\n",
    "            for col in ['title', 'author', 'text']:\n",
    "                if col in df.columns: text_parts.append(df[col].fillna(''))\n",
    "            df['text'] = pd.concat(text_parts, axis=1).agg(' '.join, axis=1)\n",
    "            all_dataframes.append(df[['text', 'label']])\n",
    "            processed_log.append(fp)\n",
    "        else:\n",
    "            error_log.append((fp, \"No 'label' column found\"))\n",
    "    except Exception as e:\n",
    "        error_log.append((fp, e))\n",
    "\n",
    "# --- Step 3: Combine, Clean, and Balance ---\n",
    "if not all_dataframes:\n",
    "    print(\"\\nCRITICAL: No data was processed. Please check file names.\")\n",
    "else:\n",
    "    print(\"\\nCombining all processed data...\")\n",
    "    df_master = pd.concat(all_dataframes, ignore_index=True)\n",
    "    df_master.dropna(subset=['text'], inplace=True)\n",
    "    df_master = df_master[df_master['text'].str.strip() != '']\n",
    "    df_master.drop_duplicates(subset=['text'], inplace=True)\n",
    "    \n",
    "    print(\"\\n--- Balancing the Dataset (Downsampling) ---\")\n",
    "    misinfo_df = df_master[df_master['label'] == 0]\n",
    "    credible_df = df_master[df_master['label'] == 1]\n",
    "    \n",
    "    if len(credible_df) > len(misinfo_df):\n",
    "        credible_downsampled_df = credible_df.sample(n=len(misinfo_df), random_state=42)\n",
    "        df_balanced = pd.concat([misinfo_df, credible_downsampled_df])\n",
    "    else: # In case there's more misinformation, which is unlikely but safe to handle\n",
    "        misinfo_downsampled_df = misinfo_df.sample(n=len(credible_df), random_state=42)\n",
    "        df_balanced = pd.concat([misinfo_downsampled_df, credible_df])\n",
    "\n",
    "    df_final_training = df_balanced.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Final Balanced Training Set Size: {len(df_final_training)}\")\n",
    "    print(f\"CREDIBLE articles in final set: {df_final_training['label'].value_counts().get(1, 0)}\")\n",
    "    print(f\"MISINFORMATION articles in final set: {df_final_training['label'].value_counts().get(0, 0)}\")\n",
    "\n",
    "    # --- Step 4: Save the Final Dataset ---\n",
    "    print(\"\\nSaving the final, balanced training dataset...\")\n",
    "    df_final_training.to_csv('master_balanced_training_data.csv', index=False)\n",
    "    print(\"SUCCESS: File 'master_balanced_training_data.csv' saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6e538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
